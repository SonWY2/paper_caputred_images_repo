https://arxiv.org/pdf/2307.14936.pdf

[요약]
- 코드 생성을 위해 사전 학습된 대규모 언어 모델을 효과적이고 효율적으로 향상시킬 RRTF(Rank Responses to align Test&Teacher Feedback) 프레임워크 제안
- stacoder-15B를 RRTF를 활용한 강화학습으로 HumanEval, CoderEval, LeetCode 벤치마크에서 높은 성능 달성

[기여]
- 코드 작업에 대하여 데이터 효율적이고 구현하기 쉬우며 모델에 구애받지 않는 RRTF 패러다임 제안
- 기본 모델보다 30% 가까이 개선된 모델 구현
- 효과적인 학습 데이터 구축, RRTF 모델 학습, 모델 최적화 경험 및 연구결과 공유



- 기존 RL기반 접근방식의 문제
  - 기존 RL은 compiler, debugger, executor, test case와 같은 코드 프로세서의 피드백에 따라 가치/보상 함수 설계함
  - 테스트 결과를 보상으로 직접 고려하면 기본 모델 개선에 한계 有
  - 기존 RL 알고리즘(PPO)은 구현이 복잡하고 대규모 언어 모델에서 훈련이 어려움
  - 모델 훈련하고 테스트 실행동안 많은 시간이 소요됨

-RRTF: (https://junbuml.ee/rrhf)
  - response의 ranking을 사용하여 더 간단하고 효율적인 훈련 접근방법 구현

[RRTF]
- Instruction Tuning, Evol-Instruct, 강화 학습을 결합한 학습 프로세스 & 프레임워크 제안
- 핵심 아이디어: 테스트 response와 사람의 선호도를 함께 활용하여 응답의 순위를 매기는 feedback으로 활용.
https://github-production-user-asset-6210df.s3.amazonaws.com/36894403/257054667-08abf7ec-5145-4f0c-aa9f-00b4f0a9d926.png
- Human(인간)을 사람 의도에 맞추는 대신, test와 teacher(더 강력한 LLM)의 조합인 T로 대체하면 공동의 피드백 신호로 코드 LLM학습 가능
- 이 과정은 완전 자동화/반자동화 시킬 수 있으므로 더 빠르고 효율적으로 학습 가능
  - Sampling 단계: Evol-Instruct를 통해 생성된 프롬프트를 활용하여 응답 샘플링
    - student모델과 teacher 모델 모두에서 다양한 온도별 응답을 샘플링.
  - Ranking 단계: 단위 테스트 및 휴리스틱 설정에 따라 응답에 랭킹
    - unit test와 휴리스틱 선호도에 따라 다양한 소스의 응답에 순위 매김. 
    - 모든 응답 수집 후 병렬로 실행환경에서 실행한 테스트 결과를 수집(compile error, runtime error, pass partial test, all pass)
    - 각 케이스에 대해 테스트 결과에 따라 점수 부여. 이 때, teacher 모델의 점수가 student 모델보다 낮은 경우 필터링.
  - Traning 단계: 세가지의 프롬프트와 점수와 함께 제공되는 선택/거부된 응답을 통해 code LLM 학습
    - (prompt, chosen_response+score, rejected_response+score) 로 구성된 트리펫으로 학습수행.
    - 각 prompt에 대해 teacher모델의 y와 student모델의 y 응답쌍이 있으며 조건부 로그 확률로 나타냄
    https://user-images.githubusercontent.com/36894403/257142273-a2d98173-b9f1-4ae1-aa8f-7857ac77e463.png
    https://user-images.githubusercontent.com/36894403/257142325-a67bad52-17f8-47c0-bf91-ebd102ef9cc8.png
    https://user-images.githubusercontent.com/36894403/257142391-4bcacd51-52f3-4c58-acfd-11e25af2c93d.png
    https://user-images.githubusercontent.com/36894403/257142436-24240cc6-ef91-4668-99d7-30749ed50177.png

[Architecture]
- Multi-query-attention, absolute positional embedding, flash attention
https://github-production-user-asset-6210df.s3.amazonaws.com/36894403/257054907-8325d3fe-5f53-4aee-ad3f-05d908bf4e45.png

[dataset]
- code alpaca 20k 데이터셋에서 evol-instruct를 활용해 데이터셋을 진화시킴(반복적으로 수행)
- 프롬프트
  https://github-production-user-asset-6210df.s3.amazonaws.com/36894403/257057740-e2920e78-4874-4f08-91c5-31bef0604405.png
- 다양한 모델에서 해당 문제의 답 샘플링함. => 10만개 insturction-solution 데이터 쌍(초기 말뭉치) 수집


[결과]
https://github-production-user-asset-6210df.s3.amazonaws.com/36894403/257060981-9b0b3e93-f14f-4792-be8c-b119d328d916.png
- 모든 opensoure LLM 중에 가장 우수한 성능.
- basemodel인 StarCoder와 비교했을 때, 28p% 향상.
https://github-production-user-asset-6210df.s3.amazonaws.com/36894403/257061032-9a998e3e-4259-416c-8a73-a4b2646ddaa6.png
- greedy decoding 방식에서도 가장 우수한 성능 발휘.

[Findings]
- 전체 데이터 세트 크기가 커질수록 전체 정확도도 증가
- 큰 데이터세트(38k, 68k)에서 2~3 에포크 후 안정화 되었음
- 데이터세트 규모가 적을 경우 안정화가 느리고 잘 되지 않음.
- 데이터세트가 커져도 최대 성능에 도달하는데 필요한 epoch수가 증가하지는 않음.

- pangu-coder2와 wizardcoder는 상호 보완적이었음.(pangu-coder2가 해결할 수 없었던 문제를 wizardcoder가 해결함)
  - RL로 해결되지 않은 문제가 instruction tuning으로 해결 가능한 사례. 향후 검토 필요
- 200번의 샘플링 시에도 어떠한 모델로도 해결되지 않은 문제도 있었음.

[inference optimization]
- CTranslate2: OpenNMT에서 개발한 Transformer 모델 가속화 라이브러리
- GPTQ: LLM quantization 라이브러리
https://github-production-user-asset-6210df.s3.amazonaws.com/36894403/257061659-7af27202-0b5d-4170-9e1a-3e72295ae4eb.png

