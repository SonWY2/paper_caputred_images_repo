Hover over this sentence to see the translation. [번역](# "이 문장 위에 마우스를 올리면 번역을 볼 수 있습니다.")
Click here to see the translation. [한글 해석 보기](https://example.com/translation)


💡 Tip
> It introduces phi-1, a model with 1.3B parameters that obtains a pass@1 rate of 50.6% on HumanEval thanks to a novel training process. Unfortunately, the weights are not available.  
새로운 훈련 프로세스를 통해 13억 개의 파라미터를 가진 모델인 phi-1을 도입하여 HumanEval에서 50.6%의 합격률을 달성했습니다. 아쉽게도 가중치는 사용할 수 없습니다.


📝 Paper: https://arxiv.org/pdf/2306.11644.pdf

The authors argue that high quality data can change the shape of the scaling laws, allowing small models to match the performance of bigger ones.  
저자는 고품질 데이터가 스케일링 법칙의 형태를 변화시켜 작은 모델도 큰 모델의 성능에 맞출 수 있다고 주장합니다.
![image](https://github.com/SonWY2/paper_caputred_images_repo/assets/36894403/3bc8b10b-13bc-4f40-96f2-c5a4c014e37c)



# The importance of high-quality data


**Motivation**: The authors observe that standard code datasets like The Stack, StackOverflow and CodeContests suffer from several drawbacks: samples are not self-contained but referenced, a lot of them are trivial while the most complex ones are poorly documented, and the overall distribution is skewed towards certain topics and use cases.  
**Motivation**: 저자들은 The Stack, StackOverflow, CodeContests와 같은 표준 코드 데이터 세트가 몇 가지 단점을 가지고 있다고 지적합니다. 샘플이 독립적이지 않고 참조되고, 많은 샘플이 사소한 반면 가장 복잡한 샘플은 제대로 문서화되어 있지 않으며, 전반적인 분포가 특정 주제와 사용 사례에 치우쳐 있다는 점입니다.


They train their solution (phi-1) on a new dataset (<7B tokens) as follows:  
다음과 같이 새로운 데이터 세트(7B 토큰 미만)에 대해 솔루션(phi-1)을 훈련합니다:


**Pretraining** on CodeTextbook, comprised of a filtered version of The Stack and StackOverflow(~6B tokens) + a synthetic samples generated by GPT-3.5 (<1B tokens)  
**Fine-tuning** on CodeExercises, a small synthetic dataset of Python exercises and solutions also generated by GPT-3.5 (~180M tokens)  
필터링된 버전의 더 스택 및 스택오버플로우(~60억 토큰) + GPT-3.5에서 생성한 합성 샘플(<10억 토큰)로 구성된 CodeTextbook에 대한 **사전 교육**.  
GPT-3.5(~1억 8천만 토큰)에서 생성된 파이썬 연습 및 솔루션의 소규모 합성 데이터 세트인 CodeExercises의 **파인 튜닝**.




# Filtering code
The authors use the Python subset of the deduplicated version of The Stack and StackOverflow (35B tokens). They used **GPT-4** to annotate the quality of a subset (100k samples), using the following prompt: “determine its educational value for a student whose goal is to learn basic coding concepts.”  
저자들은 중복 제거된 버전의 더 스택과 스택오버플로우(35B 토큰)의 파이썬 하위 집합을 사용했습니다. 이들은 **GPT-4**를 사용하여 다음과 같은 프롬프트를 사용하여 하위 집합(10만 개의 샘플)의 품질에 주석을 달았습니다: "기본 코딩 개념을 배우는 것이 목표인 학생을 위한 교육적 가치를 결정하세요."


💡 Tip
> This prompt could probably be improved by asking GPT-4 to break down its reasoning into steps before outputting the final value.  
> 이 프롬프트는 최종 값을 출력하기 전에 GPT-4에 추론을 여러 단계로 세분화하도록 요청하여 개선할 수 있습니다.

This creates a dataset of code snippets and corresponding values. The authors produce embeddings of each code snippet using a pretrained CodeGen model, and train a random forest classifier to predict the quality of each sample.  
이렇게 하면 코드 스니펫과 해당 값의 데이터 세트가 생성됩니다. 작성자는 사전 학습된 CodeGen 모델을 사용하여 각 코드 조각의 임베딩을 생성하고 랜덤 포레스트 분류기를 학습하여 각 샘플의 품질을 예측합니다.

# Generating synthetic data
The authors argue the synthetic samples should be diverse (concepts, skills, scenarios, difficulty, complexity, style) and non-repetitive to reduce the risk of overfitting/memorizing and be more robust. Inspired by TinyStories, they use randomized seeds à la Alpaca to generate samples with GPT-3.5:  
저자는 합성 샘플이 다양해야 하며(개념, 기술, 시나리오, 난이도, 복잡성, 스타일), 반복적이지 않아야 과적합/암기 위험을 줄이고 더 강력하다고 주장합니다. TinyStories에서 영감을 받아 알파카의 무작위 시드를 사용하여 GPT-3.5로 샘플을 생성합니다:


Synthetic training data (<1B tokens): code and text with examples and constraints.  
CodeExercises (~180M tokens): Python exercises and solutions, where each exercise is a docstring of a function that needs to be completed.  
합성 훈련 데이터(<1B 토큰): 예제와 제약 조건이 포함된 코드와 텍스트.  
코드 연습(~1억 8천만 토큰): Python 연습 문제 및 솔루션으로, 각 연습 문제는 완료해야 하는 함수의 문서 문자열입니다.

# Model architecture and training
phi-1 is a decoder-only transformer using rotary position embedding, FlashAttention and multi-head attention (MHA), with parallel MHA and MLP layers + codegen-350M-mono’s tokenizer.  
phi-1은 회전 위치 임베딩, 플래시어텐션, 멀티헤드 어텐션(MHA)을 사용하는 디코더 전용 트랜스포머로 병렬 MHA 및 MLP 레이어 + 코드젠-350M-mono의 토큰라이저가 탑재되어 있습니다.

💡Tip
> Its architecture is very much inspired by CodeGen and does not include Fill-In-the-Middle or Multi-Query-Attention like StarCoder (low-hanging fruit improvement).  
>의 아키텍처는 CodeGen에서 매우 많은 영감을 받았으며 StarCoder와 같은 중간 채우기 또는 다중 쿼리 주의(낮은 매달린 열매 개선)를 포함하지 않습니다.


Hyperparameters for phi-1 with 1.3B/350M parameters:

24/20 layers  
Hidden dimension = 2048/1024  
MLP-inner dimension = 8192/4096  
32/16 attention heads with dimension = 64/32  
Sequence length = 2048  
Objective = next-token prediction


# The importance of fine-tuning
![image](https://github.com/SonWY2/paper_caputred_images_repo/assets/36894403/05e4d8c3-26b6-44ee-9a42-fac4d0646462)


Fine-tuning phi-1 on CodeExercises greatly improves the model’s performance, even for tasks that are not in the fine-tuning dataset.  
CodeExercises에서 phi-1을 미세 조정하면 미세 조정 데이터 세트에 없는 작업의 경우에도 모델의 성능이 크게 향상됩니다.

The authors notice that the model gets better at interpreting questions and logical relationships in the prompts. Interestingly, it also becomes better at using external libraries, even when they do not appear in the fine-tuning set (e.g., Pygame and Tkinter).  
저자들은 이 모델이 프롬프트의 질문과 논리적 관계를 해석하는 데 더 능숙해진다는 사실을 발견했습니다. 흥미롭게도 이 모델은 미세 조정 세트에 포함되지 않은 외부 라이브러리(예: Pygame 및 Tkinter)를 사용하는 데도 더 능숙해졌습니다.


# Performance evaluation
The authors argue HumanEval’s binary score (code passes the unit tests, or it fails) does not capture the nuances of the model’s performance. They introduce an LLM grading using GPT-4 (between 0 and 10), which does not require tests.  
저자들은 HumanEval의 이진 점수(코드가 단위 테스트를 통과하면 합격, 그렇지 않으면 불합격)가 모델 성능의 미묘한 차이를 포착하지 못한다고 주장합니다. 이들은 테스트가 필요 없는 GPT-4(0~10점 사이)를 사용하는 LLM 등급을 소개합니다.
![image](https://github.com/SonWY2/paper_caputred_images_repo/assets/36894403/c6f96e01-d77c-47cf-9082-fce895d221a8)



There’s a concern that CodeExercises might contain samples that are similar to exercises in HumanEval. The authors propose to remove these samples and retrain phi-1 on this decontaminated set.  
CodeExercises에 HumanEval의 연습과 유사한 샘플이 포함될 수 있다는 우려가 있습니다. 저자들은 이러한 샘플을 제거하고 오염이 제거된 이 세트에 대해 phi-1을 재훈련할 것을 제안합니다.

They report no meaningful n-gram overlap between CodeExercises and HumanEval (4 false positives). They then use a combination of embedding and syntax-based distances to find similar code snippets:  
이들은 CodeExercises와 HumanEval 사이에 의미 있는 n-그램 중복이 없다고 보고합니다(오탐 4건). 그런 다음 임베딩과 구문 기반 거리의 조합을 사용하여 유사한 코드 조각을 찾습니다:


Semantics: They use the L2 distance between embeddings produced by CodeGen  
Syntax: They calculate the (string) edit distance between the abstract syntax trees (ASTs) of two code snippets.  
Despite this data pruning, the authors claim that phi-1 still outperforms StarCoder on HumanEval.  
시맨틱: CodeGen에서 생성한 임베딩 간의 L2 거리를 사용합니다.  
Syntax: 두 코드 스니펫의 추상 구문 트리(AST) 사이의 (문자열) 편집 거리를 계산합니다.  
이러한 데이터 가지치기에도 불구하고, 저자들은 phi-1이 여전히 HumanEval에서 StarCoder보다 성능이 뛰어나다고 주장합니다.  
구문: 두 코드 스니펫의 추상 구문 트리(AST) 사이의 (문자열) 편집 거리를 계산합니다.  
이러한 데이터 가지치기에도 불구하고, 저자들은 phi-1이 여전히 HumanEval에서 StarCoder보다 성능이 뛰어나다고 주장합니다.  
