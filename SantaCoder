
[요약]
- BIGCODE는 코드용 대규모 언어 모델의 책임감 있는 개발을 위해 노력하는 개방형 과학 협업 커뮤니티
- 법률, 윤리, 데이터 라이선스, 개인 식별 정보, 악성 코드 출력 등의 위험에 대해 연구함
- 384개 프로그래밍 언어로 구성된 6.4TB 데이터 세트 출시(Stack v1.1. 현재 v1.2까지 배포)
  - https://huggingface.co/datasets/bigcode/the-stack
  - https://github.com/bigcode-project/bigcode-dataset
- PII redaction pipeline을 통한 이메일, IP주소, 비밀 키등을 탐지하여 익명화
- architecture, data preprocessing filter 측면에서 각종 ablation 실험 진행
- abalation을 통해 얻은 insight를 통해 학습한 1.1B 모델은 기존의 공개 모델들의 성능보다 뛰어남


[PII Benchmark]; 코드 내 개인정보 식별을 위한 방법
- The stack의 데이터셋에서 개인 식별 정보를 포함할 가능성이 있는 400개의 샘플 필터링하여 구성
- PII redaction pipeline을 통해 이메일은 임의의 문자열로 대체하고, IP주소는 무작위로 생성된 5개의 IP주소로 대체, 비밀 키를 코드에서 삭제 수행
- TheStack 데이터에서의 탐지 결과
https://user-images.githubusercontent.com/36894403/235060714-a314b919-e3ca-4909-b567-dc1e13d3b52f.png
  
  
[Abalation]
1) Architecture abalation
https://user-images.githubusercontent.com/36894403/235070926-9740860d-407f-4fae-aafe-6f51af3e3255.png
- FIM vs No-FIM: 
  - FIM을 수행한 모델의 성능이 일관되게 저하되는 것이 확인됨
  - 그러나 더 나은 일반화 성능을 보인다는 것을 발견
- Multi Query Attention vs Multi Head Attention
  - MQA는 MHA에 대비하여 성능이 약간 저하됨. 
  - 그러나 MQA 모델(11억 개)은 MHA 모델(13억 개) 에 비해 매개변수가 적으므로 공정하지 않을 수 있으며, MQA의 추론 속도 향상의 이득이 큼
  
2) Data filtering abalation
- github star : star 가 양질의 코드 지표라 가정하고 threshold 5개 이상 으로 필터링
  - 놀랍게도 Github star 필터 수행 결과가 안좋음
  - Github의 star 개수를 통한 repository를 필터링 하면 성능이 저하됨
  - 이는 데이터의 양이 많이 줄어서 일 수도 있으나, 삭제된 repository에 필요한 정보들이 있고 데이터의 품질이 저하된 것일 수 있음
- comment-to-code ratio : 좋은 코드는 문서화가 잘 되어 있다고 가정. threshold > 1% 로 설정하며 단 비율이 80% 넘는 데이터는 제거
  - HumanEval은 0~2% 향상되지만 MBPP 성능 일부 감소. 
- near-deduplication
  - MinHash + LSH with unigram. (5-gram과 0.7 jaccard 유사도 threshold가 최적이라는 것을 경험적으로 제시)
  - HumanEval, MBPP를 1~3,4% 정도 일관되게 향상시킴
- tokenizer fertility 
  - 문자 대 토큰의 비율(char#/token#). python의 경우 2.5, Java는 2.9, JavaScript는 2.6 으로 설정
  - 성능에 미치는 영향은 중립적임. 미묘함.

[최종 모델 architecture]
- 앞선 실험을 토대로 MQA + FIM + more near-deduplication 필터와 comments-to-code 필터를 사용하여 SantaCoder 모델 학습
  - 600K iteration(236B token)
  - FP16 FIM, MQA
  - num layer 24
  - num head 16
  - hiddensize 2048
  - Adam optimizer with β1=0.9, β2=0.95, ϵ=10-8, 가중치 감쇠 0.1
  - learning rate = 2*10-4, 2% warm up with cosine decay
- 학습 데이터
  - Stack v1.1에서 아래 필터링을 통해 python, java, javascript 268GB 데이터 추출하여 학습데이터로 사용
  - near-중복 제거
  - PII redaction 
  - 줄 길이 및 영숫자 비율에 따른 필터링
  - HumanEval, APPs, MBPP, MultiPL-E 에 포함된 데이터 필터링
- Tokenize
  - 49,152 vocab size. BPE 알고리즘 활용하여 2.6GB(60만 행)에 대해 학습
  
[결과]
https://user-images.githubusercontent.com/36894403/235078360-59c1c26d-0cbc-48fc-8b4b-6e07c3108570.png
모델 사이즈가 더 작음에도 불구하고 이전 오픈 소스 생성 모델보다 뛰어남
